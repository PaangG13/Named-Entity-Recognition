{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP2yFOt512/mVuc93LSvCeo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnobHN_dOvz9","executionInfo":{"status":"ok","timestamp":1702371309911,"user_tz":-480,"elapsed":12844,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"c0921712-861d-43b5-f417-27acdc3d5828"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=01688d83b103240a09b57e48d7ca2e7b60a29f5d0524717c22a0026d6afd9a72\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}],"source":["!pip install seqeval"]},{"cell_type":"markdown","source":["1. Load the data in google Drive"],"metadata":{"id":"oqW9OKH4U55_"}},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","path =\"/content/drive/My Drive/\"\n","os.chdir(path)\n","# os.listdir(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6YUzqqAFOxYC","executionInfo":{"status":"ok","timestamp":1726326641406,"user_tz":-480,"elapsed":29759,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"60dcccc4-d06a-4f88-c3f6-1c6d8e7d6bc7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["2. Process data"],"metadata":{"id":"GafY2-ucVXrk"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os\n","conll2003_path = \"/content/drive/MyDrive/nlp/A2\"\n","datasetpath= \"/content/drive/MyDrive/nlp/A2\"\n","def load_file(path = \"/train.txt\"):\n","    # Load the dataset\n","    train_sentences = []\n","    train_labels = []\n","    with open(conll2003_path + path) as f:\n","        sentence = []\n","        labels = []\n","        for line in f:\n","            line = line.strip()\n","            if line: # Split each line into four parts: word, part of speech, block, and label\n","                word, pos, chunk, label = line.split()\n","                sentence.append(word)\n","                labels.append(label)\n","            else:\n","                train_sentences.append(sentence)\n","                train_labels.append(labels)\n","                sentence = []\n","                labels = []\n","    return train_sentences, train_labels"],"metadata":{"id":"akJHQ425Oxaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len=64\n","def preproces(word2idx, tag2idx, num_tags, train_sentences,  train_labels):\n","    # Convert sentences and labels to numerical sequences\n","    x = [[word2idx[word.lower()] for word in sentence] for sentence in train_sentences]\n","    x = tf.keras.preprocessing.sequence.pad_sequences(maxlen=max_len, sequences=x, padding=\"post\", value=0)\n","    y = [[tag2idx[tag] for tag in labels] for labels in train_labels]\n","    y = tf.keras.preprocessing.sequence.pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n","    y = tf.keras.utils.to_categorical(y, num_tags)\n","    return x, y"],"metadata":{"id":"MiJw8DL5OxdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dataset():\n","    # Load the dataset\n","    train_sentences, train_labels = load_file(\"/train.txt\")\n","    valid_sentences, valid_labels = load_file(\"/valid.txt\")\n","    test_sentences, test_labels = load_file(\"/test.txt\")\n","    # Create vocabulary and tag dictionaries\n","    all_sentencses = np.concatenate([train_sentences, valid_sentences,test_sentences])\n","    all_labels = np.concatenate([train_labels, valid_labels, test_labels])\n","    vocab = set()\n","    tags = set()\n","    for sentence in all_sentencses:\n","        for word in sentence:\n","            vocab.add(word.lower())\n","    word2idx = {}\n","    if len(word2idx) == 0:\n","        word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n","        word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n","    for word in vocab:\n","        word2idx[word] = len(word2idx)\n","\n","    for labels in all_labels:\n","        for label in labels:\n","            tags.add(label)\n","    tag2idx = {t: i for i, t in enumerate(tags)}\n","    save_dict(word2idx, os.path.join(conll2003_path, 'word2idx.json'),)\n","    save_dict(tag2idx, os.path.join(conll2003_path, 'idx2Label.json'),)\n","    num_words = len(word2idx) + 1\n","    num_tags = len(tag2idx)\n","    train_X, train_y = preproces(word2idx, tag2idx, num_tags, train_sentences, train_labels);\n","    valid_X, valid_y = preproces(word2idx, tag2idx, num_tags, valid_sentences, valid_labels);\n","    test_X, test_y = preproces(word2idx, tag2idx, num_tags, test_sentences, test_labels);\n","    np.savez( os.path.join(conll2003_path ,'dataset.npz'), train_X = train_X, train_y = train_y, valid_X = valid_X, valid_y =valid_y , test_X =test_X, test_y= test_y)\n","    return train_X, train_y, valid_X, valid_y , test_X, test_y"],"metadata":{"id":"1PPzchmzOxft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_dict(dict, file_path):\n","    import json\n","    # Saving the dictionary to a file\n","    with open(file_path, 'w') as f:\n","        json.dump(dict, f)\n","def load_dict(path_file):\n","    import json\n","    # Loading the dictionary from the file\n","    with open(path_file, 'r') as f:\n","        loaded_dict = json.load(f)\n","        return loaded_dict;\n","    print(loaded_dict)"],"metadata":{"id":"WhoBQIIdOxiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_X, train_y, valid_X, valid_y , test_X, test_y=get_dataset()"],"metadata":{"id":"vMVPKeqYOxk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n","import keras as keras\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n","import numpy as np"],"metadata":{"id":"YjvhgJaMOxnh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Use GPU to accelerate"],"metadata":{"id":"m9Vmda2QWkTj"}},{"cell_type":"code","source":["if tf.test.gpu_device_name():\n","    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n","else:\n","    print(\"Please install GPU version of TensorFlow\")\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","session = tf.compat.v1.Session(config=config)\n","# physical_devices = tf.config.list_physical_devices('GPU')\n","# tf.config.experimental.set_memory_growth(physical_devices[0], True)"],"metadata":{"id":"CszWCgmNOxpt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702373985987,"user_tz":-480,"elapsed":553,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"ecbcad2b-63f0-4b68-d5e9-aeb9d879cc07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Default GPU Device: /device:GPU:0\n"]}]},{"cell_type":"markdown","source":["4. Define LSTM Model"],"metadata":{"id":"WjtSMScAWuI2"}},{"cell_type":"code","source":["def create_model():\n","    word2idx = load_dict('/content/drive/MyDrive/nlp/A2/word2idx.json')\n","    tag2idx = load_dict('/content/drive/MyDrive/nlp/A2/idx2Label.json')\n","    num_words = len(word2idx) + 1\n","    num_tags = len(tag2idx)\n","\n","    # Define the model\n","    input_layer = Input(shape=(None,))\n","    embedding_layer = Embedding(input_dim=num_words, output_dim=60, input_length=max_len)(input_layer)\n","    lstm_layer = LSTM(units=50, return_sequences=True, dropout=0.5)(embedding_layer)\n","    #lstm_layer2 = LSTM(units=50, return_sequences=True, dropout=0.5)(lstm_layer)\n","    #lstm_layer3 = LSTM(units=50, return_sequences=True, dropout=0.5)(lstm_layer2)\n","    output_layer = TimeDistributed(Dense(num_tags, activation=\"softmax\"))(lstm_layer)\n","\n","    model = Model(input_layer, output_layer)\n","\n","    return model"],"metadata":{"id":"fwiSad9_OxsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train( model,  train_X, train_y, valid_X, valid_y):\n","    # Define the path and file name for saving the model\n","    model_path = '/content/drive/MyDrive/nlp/A2/lstm.h5'\n","    # Define early stop callback function\n","    early_stop = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', verbose=1)\n","    # Define the ModelCheckpoint callback function\n","    checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n","    # Compile and train the model\n","    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    model.fit(train_X, train_y, batch_size=32, epochs=20, validation_data=(valid_X, valid_y), callbacks=[early_stop, checkpoint])"],"metadata":{"id":"xFoPCIUROxu5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(test_X, test_y ):\n","    model = keras.models.load_model('/content/drive/MyDrive/nlp/A2/lstm.h5')\n","    # Evaluation Model\n","    scores = model.evaluate(test_X, test_y, verbose=0)\n","    print(\"Test Accuracy:\", scores[1])\n","    #print(scores)"],"metadata":{"id":"S371QsWgOxxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Train & Eval the Model"],"metadata":{"id":"yHRUp7wIXQTa"}},{"cell_type":"code","source":["model= create_model()\n","train(model, np.concatenate([train_X, valid_X]), np.concatenate([train_y, valid_y]),test_X, test_y)\n","test(test_X, test_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"opKZv6nSP8s2","executionInfo":{"status":"ok","timestamp":1702376837672,"user_tz":-480,"elapsed":122502,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"973cf391-947a-4469-9fcc-c37328b25bb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","577/577 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.9626\n","Epoch 1: val_accuracy improved from -inf to 0.96858, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 35s 56ms/step - loss: 0.2148 - accuracy: 0.9626 - val_loss: 0.1044 - val_accuracy: 0.9686\n","Epoch 2/20\n","577/577 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9727\n","Epoch 2: val_accuracy improved from 0.96858 to 0.97257, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 9s 15ms/step - loss: 0.0861 - accuracy: 0.9727 - val_loss: 0.0836 - val_accuracy: 0.9726\n","Epoch 3/20\n","577/577 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9787\n","Epoch 3: val_accuracy improved from 0.97257 to 0.97618, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 9s 15ms/step - loss: 0.0656 - accuracy: 0.9787 - val_loss: 0.0754 - val_accuracy: 0.9762\n","Epoch 4/20\n","577/577 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9847\n","Epoch 4: val_accuracy improved from 0.97618 to 0.97848, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 5s 9ms/step - loss: 0.0502 - accuracy: 0.9847 - val_loss: 0.0682 - val_accuracy: 0.9785\n","Epoch 5/20\n","577/577 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9900\n","Epoch 5: val_accuracy improved from 0.97848 to 0.98097, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 8s 14ms/step - loss: 0.0361 - accuracy: 0.9900 - val_loss: 0.0604 - val_accuracy: 0.9810\n","Epoch 6/20\n","577/577 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9929\n","Epoch 6: val_accuracy did not improve from 0.98097\n","577/577 [==============================] - 5s 9ms/step - loss: 0.0260 - accuracy: 0.9929 - val_loss: 0.0611 - val_accuracy: 0.9802\n","Epoch 7/20\n","575/577 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9941\n","Epoch 7: val_accuracy improved from 0.98097 to 0.98257, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 7s 11ms/step - loss: 0.0207 - accuracy: 0.9941 - val_loss: 0.0550 - val_accuracy: 0.9826\n","Epoch 8/20\n","575/577 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9948\n","Epoch 8: val_accuracy improved from 0.98257 to 0.98273, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 5s 9ms/step - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.0566 - val_accuracy: 0.9827\n","Epoch 9/20\n","577/577 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9954\n","Epoch 9: val_accuracy did not improve from 0.98273\n","577/577 [==============================] - 5s 9ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0584 - val_accuracy: 0.9823\n","Epoch 10/20\n","577/577 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9958\n","Epoch 10: val_accuracy improved from 0.98273 to 0.98306, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 7s 13ms/step - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.0579 - val_accuracy: 0.9831\n","Epoch 11/20\n","570/577 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9961\n","Epoch 11: val_accuracy improved from 0.98306 to 0.98313, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 4s 8ms/step - loss: 0.0126 - accuracy: 0.9961 - val_loss: 0.0587 - val_accuracy: 0.9831\n","Epoch 12/20\n","574/577 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9964\n","Epoch 12: val_accuracy improved from 0.98313 to 0.98342, saving model to /content/drive/MyDrive/nlp/A2/lstm.h5\n","577/577 [==============================] - 5s 8ms/step - loss: 0.0115 - accuracy: 0.9964 - val_loss: 0.0598 - val_accuracy: 0.9834\n","Epoch 13/20\n","577/577 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9966\n","Epoch 13: val_accuracy did not improve from 0.98342\n","577/577 [==============================] - 6s 10ms/step - loss: 0.0106 - accuracy: 0.9966 - val_loss: 0.0619 - val_accuracy: 0.9834\n","Epoch 14/20\n","573/577 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9967\n","Epoch 14: val_accuracy did not improve from 0.98342\n","577/577 [==============================] - 5s 8ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 0.0623 - val_accuracy: 0.9833\n","Epoch 15/20\n","573/577 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9970\n","Epoch 15: val_accuracy did not improve from 0.98342\n","577/577 [==============================] - 5s 8ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.0625 - val_accuracy: 0.9831\n","Epoch 15: early stopping\n","Test Accuracy: 0.983424961566925\n"]}]},{"cell_type":"markdown","source":["6. Predict the Labels"],"metadata":{"id":"BfH6_UEEX5sX"}},{"cell_type":"code","source":["predictions = model.predict(test_X)\n","# Convert predicted tags back to labels\n","predicted_labels = []\n","predicted_tags = tf.argmax(predictions, axis=-1)\n","word2idx = load_dict('/content/drive/MyDrive/nlp/A2/word2idx.json')\n","tag2idx = load_dict('/content/drive/MyDrive/nlp/A2/idx2Label.json')\n","for tags in predicted_tags:\n","    labels = [list(tag2idx.keys())[tag] for tag in tags if tag != 0]\n","    predicted_labels.append(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BAeGzm7LP8qF","executionInfo":{"status":"ok","timestamp":1702377005391,"user_tz":-480,"elapsed":151717,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"16434e3e-8a3d-425b-ef55-e3bb0ccb5e76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["116/116 [==============================] - 1s 3ms/step\n"]}]},{"cell_type":"code","source":["test_sentences, test_labels = load_file(\"/test.txt\")\n","predicted_labels2=predicted_labels\n","# Unify the dimension\n","for i in range(len(test_labels)):\n","  test_labels[i]=test_labels[i][:64]\n","  predicted_labels2[i]=predicted_labels2[i]+['O']*(64-len(predicted_labels2[i]))\n","  predicted_labels2[i]=predicted_labels2[i][:len(test_labels[i])]"],"metadata":{"id":"_fsTnZ1wP8n0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. Use Segeval to Evaluate the Tagger on the Test Set"],"metadata":{"id":"lMWgjJWRYPCQ"}},{"cell_type":"code","source":["from seqeval.metrics import accuracy_score\n","from seqeval.metrics import classification_report\n","from seqeval.metrics import f1_score\n","from seqeval.metrics import accuracy_score,precision_score,recall_score\n","print(\"Accuracy Score : \",accuracy_score(test_labels, predicted_labels2))\n","print(\"Precision Score : \",precision_score(test_labels, predicted_labels2))\n","print(\"Recall Score : \",recall_score(test_labels, predicted_labels2))\n","print(\"F1 Score : \",f1_score(test_labels, predicted_labels2))\n","print(\"-\"*30)\n","print(\"Classification_Report\")\n","print(classification_report(test_labels, predicted_labels2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWRj8_xtP8la","executionInfo":{"status":"ok","timestamp":1702377015204,"user_tz":-480,"elapsed":1047,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"98da6bea-288b-4982-d842-df2f41a61019"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score :  0.9013639780904307\n","Precision Score :  0.56312625250501\n","Recall Score :  0.5993601137575542\n","F1 Score :  0.580678491475805\n","------------------------------\n","Classification_Report\n","              precision    recall  f1-score   support\n","\n","         LOC       0.66      0.70      0.68      1661\n","        MISC       0.56      0.55      0.56       702\n","         ORG       0.42      0.59      0.49      1661\n","         PER       0.71      0.52      0.60      1602\n","\n","   micro avg       0.56      0.60      0.58      5626\n","   macro avg       0.59      0.59      0.58      5626\n","weighted avg       0.59      0.60      0.59      5626\n","\n"]}]},{"cell_type":"markdown","source":["8. Save the Prediction"],"metadata":{"id":"jVZ3X6i1Y6tL"}},{"cell_type":"code","source":["ts, test_labels = load_file(\"/test.txt\")\n","text=''\n","for i in range(len(ts)):\n","  for j in range(len(ts[i])):\n","    a=''\n","    if(j>=len(predicted_labels[i])):\n","      a=ts[i][j]+' O\\n'\n","    else:\n","      a=ts[i][j]+' '+predicted_labels[i][j]+'\\n'\n","    text=text+a\n","  text=text+'\\n'\n","#print(text)"],"metadata":{"id":"Q5c_eVUYP8i8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/nlp/A2/lstm.test.txt\", \"w\") as file:\n","  file.write(text)"],"metadata":{"id":"H_L0mZMIP8gX"},"execution_count":null,"outputs":[]}]}