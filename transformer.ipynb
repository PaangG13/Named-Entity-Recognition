{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNb48jpx8LmtswNpx3hfQ7X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnobHN_dOvz9","executionInfo":{"status":"ok","timestamp":1702396894042,"user_tz":-480,"elapsed":6032,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"7cdd5665-ff7e-46ce-dbfc-21fbdc3a5570"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"]}],"source":["!pip install seqeval"]},{"cell_type":"markdown","source":["1. Load the data in google Drive"],"metadata":{"id":"oqW9OKH4U55_"}},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","path =\"/content/drive/My Drive/\"\n","os.chdir(path)\n","# os.listdir(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6YUzqqAFOxYC","executionInfo":{"status":"ok","timestamp":1726326732303,"user_tz":-480,"elapsed":17075,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"f53cc9f2-4121-45e9-92ab-cb215deb4631"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["2. Process data"],"metadata":{"id":"GafY2-ucVXrk"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os\n","conll2003_path = \"/content/drive/MyDrive/nlp/A2\"\n","datasetpath= \"/content/drive/MyDrive/nlp/A2\"\n","def load_file(path = \"/train.txt\"):\n","    # Load the dataset\n","    train_sentences = []\n","    train_labels = []\n","    with open(conll2003_path + path) as f:\n","        sentence = []\n","        labels = []\n","        for line in f:\n","            line = line.strip()\n","            if line: # Split each line into four parts: word, part of speech, block, and label\n","                word, pos, chunk, label = line.split()\n","                sentence.append(word)\n","                labels.append(label)\n","            else:\n","                train_sentences.append(sentence)\n","                train_labels.append(labels)\n","                sentence = []\n","                labels = []\n","    return train_sentences, train_labels"],"metadata":{"id":"akJHQ425Oxaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len=64\n","def preproces(word2idx, tag2idx, num_tags, train_sentences,  train_labels):\n","    # Convert sentences and labels to numerical sequences\n","    x = [[word2idx[word.lower()] for word in sentence] for sentence in train_sentences]\n","    x = tf.keras.preprocessing.sequence.pad_sequences(maxlen=max_len, sequences=x, padding=\"post\", value=0)\n","    y = [[tag2idx[tag] for tag in labels] for labels in train_labels]\n","    y = tf.keras.preprocessing.sequence.pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n","    y = tf.keras.utils.to_categorical(y, num_tags)\n","    return x, y"],"metadata":{"id":"MiJw8DL5OxdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dataset():\n","    # Load the dataset\n","    train_sentences, train_labels = load_file(\"/train.txt\")\n","    valid_sentences, valid_labels = load_file(\"/valid.txt\")\n","    test_sentences, test_labels = load_file(\"/test.txt\")\n","    # Create vocabulary and tag dictionaries\n","    all_sentencses = np.concatenate([train_sentences, valid_sentences,test_sentences])\n","    all_labels = np.concatenate([train_labels, valid_labels, test_labels])\n","    vocab = set()\n","    tags = set()\n","    for sentence in all_sentencses:\n","        for word in sentence:\n","            vocab.add(word.lower())\n","    word2idx = {}\n","    if len(word2idx) == 0:\n","        word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n","        word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n","    for word in vocab:\n","        word2idx[word] = len(word2idx)\n","\n","    for labels in all_labels:\n","        for label in labels:\n","            tags.add(label)\n","    tag2idx = {t: i for i, t in enumerate(tags)}\n","    save_dict(word2idx, os.path.join(conll2003_path, 'word2idx.json'),)\n","    save_dict(tag2idx, os.path.join(conll2003_path, 'idx2Label.json'),)\n","    num_words = len(word2idx) + 1\n","    num_tags = len(tag2idx)\n","    train_X, train_y = preproces(word2idx, tag2idx, num_tags, train_sentences, train_labels);\n","    valid_X, valid_y = preproces(word2idx, tag2idx, num_tags, valid_sentences, valid_labels);\n","    test_X, test_y = preproces(word2idx, tag2idx, num_tags, test_sentences, test_labels);\n","    np.savez( os.path.join(conll2003_path ,'dataset.npz'), train_X = train_X, train_y = train_y, valid_X = valid_X, valid_y =valid_y , test_X =test_X, test_y= test_y)\n","    return train_X, train_y, valid_X, valid_y , test_X, test_y"],"metadata":{"id":"1PPzchmzOxft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_dict(dict, file_path):\n","    import json\n","    # Saving the dictionary to a file\n","    with open(file_path, 'w') as f:\n","        json.dump(dict, f)\n","def load_dict(path_file):\n","    import json\n","    # Loading the dictionary from the file\n","    with open(path_file, 'r') as f:\n","        loaded_dict = json.load(f)\n","        return loaded_dict;\n","    print(loaded_dict)"],"metadata":{"id":"WhoBQIIdOxiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_X, train_y, valid_X, valid_y , test_X, test_y=get_dataset()"],"metadata":{"id":"vMVPKeqYOxk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Model\n","from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n","import keras as keras\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n","import numpy as np"],"metadata":{"id":"YjvhgJaMOxnh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Use GPU to accelerate"],"metadata":{"id":"m9Vmda2QWkTj"}},{"cell_type":"code","source":["if tf.test.gpu_device_name():\n","    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n","else:\n","    print(\"Please install GPU version of TensorFlow\")\n","\n","config = tf.compat.v1.ConfigProto()\n","config.gpu_options.allow_growth = True\n","session = tf.compat.v1.Session(config=config)\n","tf.config.experimental_run_functions_eagerly(True)\n","# physical_devices = tf.config.list_physical_devices('GPU')\n","# tf.config.experimental.set_memory_growth(physical_devices[0], True)"],"metadata":{"id":"CszWCgmNOxpt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702458546592,"user_tz":-480,"elapsed":755,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"a290f97b-b83f-42ae-9a85-8997d8ba6fed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Default GPU Device: /device:GPU:0\n"]}]},{"cell_type":"markdown","source":["4. Define Transformer Model"],"metadata":{"id":"WjtSMScAWuI2"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Dense, LayerNormalization, MultiHeadAttention, Dropout, Input\n","from tensorflow.keras.models import Model\n","\n","def Transformer(vocab_size, num_layers, d_model, num_heads, d_ff, input_length, dropout_rate):\n","\n","    word2idx = load_dict('/content/drive/MyDrive/nlp/A2/word2idx.json')\n","    tag2idx = load_dict('/content/drive/MyDrive/nlp/A2/idx2Label.json')\n","    max_len=64\n","    num_words = len(word2idx) + 1\n","    num_tags = len(tag2idx)\n","    input_layer = Input(shape=(None,))\n","    vocab_size=num_words\n","    # Input Layer\n","    inputs = input_layer\n","    # Position Encoding\n","    position_encoding = positional_encoding(input_length, d_model)\n","\n","    # Input Embedding Layer\n","    embedding = Embedding(vocab_size, d_model)(inputs)\n","    embedding *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","    embedding += position_encoding\n","\n","    # Dropout Layer\n","    x = Dropout(dropout_rate)(embedding)\n","    # Transformer Layer\n","    for _ in range(num_layers):\n","        x = encoder_layer(d_model, num_heads, d_ff, dropout_rate)(x)\n","    for _ in range(num_layers):\n","        x = decoder_layer(d_model, num_heads, d_ff, dropout_rate)(x)\n","    # Output Layer\n","    outputs = TimeDistributed(Dense(num_tags, activation=\"softmax\"))(x)\n","    # Build the Model\n","    model = Model(inputs=inputs, outputs=outputs)\n","\n","    return model\n","\n","def encoder_layer(d_model, num_heads, d_ff, dropout_rate):\n","    inputs = tf.keras.Input(shape=(None, d_model))\n","\n","    # MultiHeadAttention\n","    attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(inputs, inputs)\n","    #attention = MultiHeadAttention(num_heads=num_heads)(inputs, inputs)\n","    attention = Dropout(dropout_rate)(attention)\n","    attention = LayerNormalization(epsilon=1e-6)(inputs + attention)\n","\n","    # FFN\n","    outputs = Dense(d_ff, activation='relu')(attention)\n","    outputs = Dense(d_model)(outputs)\n","    outputs = Dropout(dropout_rate)(outputs)\n","    outputs = LayerNormalization(epsilon=1e-6)(attention + outputs)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=outputs)\n","def decoder_layer(d_model, num_heads, d_ff, dropout_rate):\n","    inputs = tf.keras.Input(shape=(None, d_model))\n","\n","    # MultiHeadAttention\n","    attention1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(inputs, inputs)\n","    attention1 = Dropout(dropout_rate)(attention1)\n","    attention1 = LayerNormalization(epsilon=1e-6)(inputs + attention1)\n","\n","    # ultiHeadAttention（Encoder-Decoder Attention）\n","    attention2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(attention1, attention1)\n","    attention2 = Dropout(dropout_rate)(attention2)\n","    attention2 = LayerNormalization(epsilon=1e-6)(attention1 + attention2)\n","\n","    # FFN\n","    outputs = Dense(d_ff, activation='relu')(attention2)\n","    outputs = Dense(d_model)(outputs)\n","    outputs = Dropout(dropout_rate)(outputs)\n","    outputs = LayerNormalization(epsilon=1e-6)(attention2 + outputs)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","def positional_encoding(max_length, d_model):\n","    pos = tf.expand_dims(tf.range(max_length, dtype=tf.float32), axis=1)\n","    div_term = tf.pow(10000, 2 * tf.range(d_model // 2, dtype=tf.float32) / d_model)\n","    encodings = tf.concat([tf.sin(pos / div_term), tf.cos(pos / div_term)], axis=1)\n","    return tf.expand_dims(encodings, axis=0)"],"metadata":{"id":"FEbVEvvS-mxe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train( model,  train_X, train_y, valid_X, valid_y):\n","    # Define the path and file name for saving the model\n","    model_path = '/content/drive/MyDrive/nlp/A2/transformer.h5'\n","    # Define early stop callback function\n","    early_stop = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', verbose=1)\n","    # Define the ModelCheckpoint callback function\n","    checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n","    # Compile and train the model\n","    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    model.fit(train_X, train_y, batch_size=32, epochs=3, validation_data=(valid_X, valid_y), callbacks=[early_stop, checkpoint])"],"metadata":{"id":"xFoPCIUROxu5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(test_X, test_y):\n","    model = keras.models.load_model('/content/drive/MyDrive/nlp/A2/transformer.h5')\n","    # Evaluation Model\n","    scores = model.evaluate(test_X, test_y, verbose=0)\n","    print(\"Test Accuracy:\", scores[1])\n","    #print(scores)"],"metadata":{"id":"S371QsWgOxxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["5. Train & Eval the Model"],"metadata":{"id":"yHRUp7wIXQTa"}},{"cell_type":"code","source":["word2idx = load_dict('/content/drive/MyDrive/nlp/A2/word2idx.json')\n","tag2idx = load_dict('/content/drive/MyDrive/nlp/A2/idx2Label.json')\n","\n","num_words = len(word2idx) + 1\n","num_tags = len(tag2idx)\n","vocab_size=num_tags\n","num_layers=2\n","d_model=64 # how long a vector to express a word\n","num_heads=8\n","d_ff=64\n","input_length=64\n","dropout_rate=0.2\n","model=Transformer(vocab_size, num_layers, d_model, num_heads, d_ff, input_length, dropout_rate)"],"metadata":{"id":"O2OUKzMf9lE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(model, np.concatenate([train_X, valid_X]), np.concatenate([train_y, valid_y]),test_X, test_y)\n","test(test_X, test_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"opKZv6nSP8s2","executionInfo":{"status":"ok","timestamp":1702396443123,"user_tz":-480,"elapsed":695635,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"b7c27c72-c0e4-4ced-bc14-e1469309824a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","577/577 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9914\n","Epoch 1: val_accuracy improved from -inf to 0.98315, saving model to /content/drive/MyDrive/nlp/A2/transformer.h5\n","577/577 [==============================] - 222s 384ms/step - loss: 0.0288 - accuracy: 0.9914 - val_loss: 0.0633 - val_accuracy: 0.9832\n","Epoch 2/3\n","577/577 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9927\n","Epoch 2: val_accuracy improved from 0.98315 to 0.98370, saving model to /content/drive/MyDrive/nlp/A2/transformer.h5\n","577/577 [==============================] - 220s 382ms/step - loss: 0.0237 - accuracy: 0.9927 - val_loss: 0.0619 - val_accuracy: 0.9837\n","Epoch 3/3\n","577/577 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9936\n","Epoch 3: val_accuracy did not improve from 0.98370\n","577/577 [==============================] - 225s 390ms/step - loss: 0.0204 - accuracy: 0.9936 - val_loss: 0.0646 - val_accuracy: 0.9832\n","Test Accuracy: 0.9837048649787903\n"]}]},{"cell_type":"markdown","source":["6. Predict the Labels"],"metadata":{"id":"BfH6_UEEX5sX"}},{"cell_type":"code","source":["predictions = model.predict(test_X)\n","# Convert predicted tags back to labels\n","predicted_labels = []\n","predicted_tags = tf.argmax(predictions, axis=-1)\n","word2idx = load_dict('/content/drive/MyDrive/nlp/A2/word2idx.json')\n","tag2idx = load_dict('/content/drive/MyDrive/nlp/A2/idx2Label.json')\n","for tags in predicted_tags:\n","    labels = [list(tag2idx.keys())[tag] for tag in tags if tag != 0]\n","    predicted_labels.append(labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BAeGzm7LP8qF","executionInfo":{"status":"ok","timestamp":1702396619728,"user_tz":-480,"elapsed":160855,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"17bf20ea-6c41-47b7-abf9-00e07de2318f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["116/116 [==============================] - 7s 61ms/step\n"]}]},{"cell_type":"code","source":["test_sentences, test_labels = load_file(\"/test.txt\")\n","predicted_labels2=predicted_labels\n","# Unify the dimension\n","for i in range(len(test_labels)):\n","  test_labels[i]=test_labels[i][:64]\n","  predicted_labels2[i]=predicted_labels2[i]+['O']*(64-len(predicted_labels2[i]))\n","  predicted_labels2[i]=predicted_labels2[i][:len(test_labels[i])]"],"metadata":{"id":"_fsTnZ1wP8n0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. Use Segeval to Evaluate the Tagger on the Test Set"],"metadata":{"id":"lMWgjJWRYPCQ"}},{"cell_type":"code","source":["from seqeval.metrics import accuracy_score\n","from seqeval.metrics import classification_report\n","from seqeval.metrics import f1_score\n","from seqeval.metrics import accuracy_score,precision_score,recall_score\n","print(\"Accuracy Score : \",accuracy_score(test_labels, predicted_labels2))\n","print(\"Precision Score : \",precision_score(test_labels, predicted_labels2))\n","print(\"Recall Score : \",recall_score(test_labels, predicted_labels2))\n","print(\"F1 Score : \",f1_score(test_labels, predicted_labels2))\n","print(\"-\"*30)\n","print(\"Classification_Report\")\n","print(classification_report(test_labels, predicted_labels2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KWRj8_xtP8la","executionInfo":{"status":"ok","timestamp":1702396670221,"user_tz":-480,"elapsed":2461,"user":{"displayName":"Paang.G","userId":"05692570142384984425"}},"outputId":"92ef17ae-ad49-48ff-a226-b515617f4250"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score :  0.8736977768231124\n","Precision Score :  0.43062286842577635\n","Recall Score :  0.42641308211873447\n","F1 Score :  0.42850763597392155\n","------------------------------\n","Classification_Report\n","              precision    recall  f1-score   support\n","\n","         LOC       0.52      0.54      0.53      1661\n","        MISC       0.54      0.60      0.57       702\n","         ORG       0.52      0.53      0.53      1661\n","         PER       0.14      0.12      0.13      1602\n","\n","   micro avg       0.43      0.43      0.43      5626\n","   macro avg       0.43      0.45      0.44      5626\n","weighted avg       0.42      0.43      0.42      5626\n","\n"]}]},{"cell_type":"markdown","source":["8. Save the Prediction"],"metadata":{"id":"jVZ3X6i1Y6tL"}},{"cell_type":"code","source":["ts, test_labels = load_file(\"/test.txt\")\n","text=''\n","for i in range(len(ts)):\n","  for j in range(len(ts[i])):\n","    a=''\n","    if(j>=len(predicted_labels[i])):\n","      a=ts[i][j]+' O\\n'\n","    else:\n","      a=ts[i][j]+' '+predicted_labels[i][j]+'\\n'\n","    text=text+a\n","  text=text+'\\n'\n","#print(text)"],"metadata":{"id":"Q5c_eVUYP8i8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/nlp/A2/transformer.test.txt\", \"w\") as file:\n","  file.write(text)"],"metadata":{"id":"H_L0mZMIP8gX"},"execution_count":null,"outputs":[]}]}